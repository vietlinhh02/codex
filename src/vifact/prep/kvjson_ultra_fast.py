from __future__ import annotations

import json
import re
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import random
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import time
import threading
from collections import defaultdict

import pandas as pd
import numpy as np
from rapidfuzz.fuzz import partial_ratio

# Try to use GPU-accelerated libraries if available
try:
    import cupy as cp
    HAS_CUPY = True
    print("üöÄ GPU acceleration available with CuPy!")
except ImportError:
    HAS_CUPY = False
    print("üíª Using CPU-only mode")

try:
    from rank_bm25 import BM25Okapi
except ImportError:
    print("Installing rank_bm25...")
    import subprocess
    subprocess.run(["pip", "install", "rank_bm25"])
    from rank_bm25 import BM25Okapi

# Pre-compile regex patterns
PARAGRAPH_SPLIT_PATTERN = re.compile(r"\n\s*\n+")
WHITESPACE_PATTERN = re.compile(r"\s+")
TOKENIZE_PATTERN = re.compile(r"[\w√Ä-·ªπ]+")

LABEL_MAP_IN = {
    "SUPPORTED": "SUPPORTED",
    "REFUTED": "REFUTED", 
    "NEI": "INSUFFICIENT",
    "INSUFFICIENT": "INSUFFICIENT",
}


def split_paragraphs(text: str) -> List[str]:
    paras = PARAGRAPH_SPLIT_PATTERN.split(text.strip())
    paras = [WHITESPACE_PATTERN.sub(" ", p).strip() for p in paras if p.strip()]
    return paras


def chunk_text(text: str, max_tokens: int = 220) -> List[str]:
    tokens = text.split()
    if len(tokens) <= max_tokens:
        return [text]
    return [" ".join(tokens[i:i + max_tokens]) 
            for i in range(0, len(tokens), max_tokens)]


def tokenize_bm25(text: str) -> List[str]:
    text_lower = text.lower()
    words = TOKENIZE_PATTERN.findall(text_lower)
    return words


def create_vocabulary_index(all_texts: List[str]) -> Dict[str, List[int]]:
    """Create inverted index for fast pre-filtering"""
    vocab_to_chunk_ids = defaultdict(list)
    
    for chunk_id, text in enumerate(all_texts):
        tokens = set(tokenize_bm25(text))  # Use set to avoid duplicates
        for token in tokens:
            vocab_to_chunk_ids[token].append(chunk_id)
    
    return dict(vocab_to_chunk_ids)


def fast_prefilter_chunks(claim_tokens: List[str], vocab_index: Dict[str, List[int]], 
                         max_candidates: int = 1000) -> List[int]:
    """Fast pre-filtering using inverted index"""
    candidate_scores = defaultdict(int)
    
    # Score chunks by number of overlapping tokens
    for token in claim_tokens:
        if token in vocab_index:
            for chunk_id in vocab_index[token]:
                candidate_scores[chunk_id] += 1
    
    # Return top candidates by overlap score
    if not candidate_scores:
        return list(range(min(max_candidates, len(vocab_index))))
    
    sorted_candidates = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)
    return [chunk_id for chunk_id, _ in sorted_candidates[:max_candidates]]


def process_claims_batch_ultra_fast(batch_data):
    """Ultra-fast claim processing with pre-filtering"""
    batch_id, claims_batch, id_to_chunks_subset, corpus_data, topk_bm25, max_prefilter = batch_data
    all_texts, all_doc_ids, doc_text_map, vocab_index = corpus_data
    
    results = []
    batch_start = time.time()
    
    for local_idx, (idx, (key, obj)) in enumerate(claims_batch):
        claim = str(obj.get("claim", "")).strip()
        verdict = str(obj.get("verdict", "INSUFFICIENT")).strip().upper()
        label = LABEL_MAP_IN.get(verdict, "INSUFFICIENT")
        evidence_text = obj.get("evidence")

        # Fast pre-filtering to reduce search space
        claim_tokens = tokenize_bm25(claim)
        candidate_chunk_ids = fast_prefilter_chunks(claim_tokens, vocab_index, max_prefilter)
        
        if len(candidate_chunk_ids) < topk_bm25:
            # If too few candidates, use all chunks
            candidate_chunk_ids = list(range(len(all_texts)))
        
        # Build mini-BM25 only for candidates
        candidate_texts = [all_texts[i] for i in candidate_chunk_ids]
        candidate_tokens = [tokenize_bm25(text) for text in candidate_texts]
        
        if candidate_tokens:
            mini_bm25 = BM25Okapi(candidate_tokens)
            scores = mini_bm25.get_scores(claim_tokens)
            
            # Rank within candidates
            if len(scores) >= topk_bm25:
                ranked_local_idx = np.argpartition(scores, -topk_bm25)[-topk_bm25:]
                ranked_local_idx = ranked_local_idx[np.argsort(scores[ranked_local_idx])[::-1]]
            else:
                ranked_local_idx = np.argsort(scores)[::-1]
            
            # Map back to global chunk ids
            ranked_doc_ids = [all_doc_ids[candidate_chunk_ids[i]] for i in ranked_local_idx]
        else:
            # Fallback to first topk_bm25 chunks
            ranked_doc_ids = all_doc_ids[:topk_bm25]

        # Gold evidence matching (only on chunks for this key)
        gold_ids = []
        if evidence_text and key in id_to_chunks_subset:
            chunks_for_key = id_to_chunks_subset[key]
            if chunks_for_key:
                chunk_texts = [text for _, text in chunks_for_key]
                chunk_ids = [did for did, _ in chunks_for_key]
                scores_fuzzy = [partial_ratio(evidence_text, text) for text in chunk_texts]
                if scores_fuzzy:
                    best_idx = max(range(len(scores_fuzzy)), key=lambda i: scores_fuzzy[i])
                    gold_ids = [chunk_ids[best_idx]]

        evidences = [{"doc_id": did, "text": doc_text_map[did]} for did in ranked_doc_ids]

        rec = {
            "claim_id": key,
            "claim": claim,
            "label": label,
            "evidences": evidences,
        }
        if gold_ids:
            rec["gold_evidence_ids"] = gold_ids
        
        results.append(rec)
        
        # Log every 50 claims within batch
        if (local_idx + 1) % 50 == 0 or (local_idx + 1) == len(claims_batch):
            elapsed = time.time() - batch_start
            rate = (local_idx + 1) / elapsed if elapsed > 0 else 0
            avg_candidates = len(candidate_chunk_ids)
            print(f"[Worker-{batch_id}] {local_idx+1}/{len(claims_batch)} claims - {rate:.1f} claims/sec - avg candidates: {avg_candidates}")
    
    batch_time = time.time() - batch_start
    print(f"[Worker-{batch_id}] üéâ Batch completed in {batch_time:.1f}s - {len(results)} claims")
    
    return results


def prepare_from_kvjson_ultra_fast(
    in_path: str,
    out_dir: str,
    max_tokens_per_chunk: int = 220,
    topk_bm25: int = 20,
    valid_ratio: float = 0.1,
    seed: int = 42,
    n_workers: Optional[int] = None,
    max_prefilter_candidates: int = 5000,  # Reduce search space dramatically
) -> Dict[str, str]:
    """
    Ultra-fast version with pre-filtering to reduce O(n√óm) complexity
    """
    if n_workers is None:
        n_workers = min(mp.cpu_count() - 2, 6)
    else:
        n_workers = min(n_workers, 6)
    
    start_time = time.time()
    in_path = str(in_path)
    out = Path(out_dir)
    out.mkdir(parents=True, exist_ok=True)

    print(f"[ultra_fast] ‚ö° ULTRA-FAST MODE with pre-filtering")
    print(f"[ultra_fast] Using {n_workers} processes, max {max_prefilter_candidates} candidates per claim")
    
    with open(in_path, "r", encoding="utf-8") as f:
        data: Dict[str, Dict] = json.load(f)
    total_items = len(data)
    print(f"[ultra_fast] Loaded {total_items} items")

    # Step 1: Process documents (same as before)
    print(f"[ultra_fast] Step 1/4: Processing documents...")
    step1_start = time.time()
    
    corpus_rows = []
    id_to_chunks = {}
    
    for idx, (key, obj) in enumerate(data.items(), 1):
        ctx = obj.get("context") or ""
        if not ctx:
            doc_id = f"{key}#p00"
            chunks = [(doc_id, "")]
        else:
            paras = split_paragraphs(ctx)
            chunks = []
            pidx = 0
            for p in paras:
                p_chunks = chunk_text(p, max_tokens=max_tokens_per_chunk)
                for ch in p_chunks:
                    doc_id = f"{key}#p{pidx:02d}"
                    chunks.append((doc_id, ch))
                    pidx += 1
            
            if not chunks:
                doc_id = f"{key}#p00"
                chunks = [(doc_id, ctx.strip())]
        
        id_to_chunks[key] = chunks
        corpus_rows.extend(chunks)
        
        if idx % 5000 == 0 or idx == total_items:
            print(f"[ultra_fast] Documents: {idx}/{total_items} ({idx/total_items*100:.1f}%)")
    
    all_doc_ids = [did for did, _ in corpus_rows]
    all_texts = [text for _, text in corpus_rows]
    
    step1_time = time.time() - step1_start
    print(f"[ultra_fast] Step 1 completed in {step1_time:.1f}s - Created {len(corpus_rows)} chunks")

    # Save corpus
    corpus_df = pd.DataFrame({"doc_id": all_doc_ids, "text": all_texts})
    corpus_csv = out / "corpus.csv"
    corpus_df.to_csv(corpus_csv, index=False)
    doc_text_map = dict(corpus_rows)

    # Step 2: Build vocabulary index for fast pre-filtering
    print(f"[ultra_fast] Step 2/4: Building vocabulary index for fast search...")
    step2_start = time.time()
    
    vocab_index = create_vocabulary_index(all_texts)
    vocab_size = len(vocab_index)
    avg_postings = np.mean([len(postings) for postings in vocab_index.values()])
    
    step2_time = time.time() - step2_start
    print(f"[ultra_fast] Step 2 completed in {step2_time:.1f}s")
    print(f"[ultra_fast] Vocabulary: {vocab_size} unique tokens, avg {avg_postings:.1f} chunks/token")
    print(f"[ultra_fast] Search space reduced from {len(all_texts)} to max {max_prefilter_candidates} per query")

    # Step 3: Ultra-fast claim processing
    reduction_factor = max_prefilter_candidates / len(all_texts)
    estimated_time = (total_items * max_prefilter_candidates / 1_000_000) * 0.1  # Much faster estimate
    
    print(f"[ultra_fast] Step 3/4: Ultra-fast claim processing...")
    print(f"[ultra_fast] üéØ Search space reduced by {(1-reduction_factor)*100:.1f}%")
    print(f"[ultra_fast] ‚è±Ô∏è  Estimated time: ~{estimated_time:.1f} minutes (vs {(total_items * len(all_texts) / 1_000_000) * 0.01:.1f} minutes original)")
    
    step3_start = time.time()
    
    claims_items = list(enumerate(data.items(), start=1))
    claim_batch_size = max(50, len(claims_items) // (n_workers * 2))
    
    # Prepare corpus data for workers
    corpus_data = (all_texts, all_doc_ids, doc_text_map, vocab_index)
    
    claim_batches = []
    for i, start_idx in enumerate(range(0, len(claims_items), claim_batch_size)):
        batch = claims_items[start_idx:start_idx + claim_batch_size]
        batch_keys = {key for _, (key, _) in batch}
        id_to_chunks_subset = {k: v for k, v in id_to_chunks.items() if k in batch_keys}
        
        claim_batches.append((
            i,  # batch_id
            batch, 
            id_to_chunks_subset, 
            corpus_data,
            topk_bm25,
            max_prefilter_candidates
        ))
    
    print(f"[ultra_fast] Created {len(claim_batches)} batches of ~{claim_batch_size} claims each")
    
    records = []
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        futures = {executor.submit(process_claims_batch_ultra_fast, batch): i for i, batch in enumerate(claim_batches)}
        
        processed_batches = 0
        processed_claims = 0
        
        for future in as_completed(futures):
            batch_idx = futures[future]
            batch_records = future.result()
            records.extend(batch_records)
            
            processed_batches += 1
            processed_claims += len(batch_records)
            elapsed = time.time() - step3_start
            rate = processed_claims / elapsed if elapsed > 0 else 0
            eta = (total_items - processed_claims) / rate if rate > 0 else 0
            
            print(f"[ultra_fast] ‚úÖ Batch {processed_batches}/{len(claim_batches)} DONE - "
                  f"Claims: {processed_claims}/{total_items} ({processed_claims/total_items*100:.1f}%) - "
                  f"‚ö° {rate:.1f} claims/sec - ‚è∞ ETA: {eta/60:.1f} min")

    step3_time = time.time() - step3_start
    print(f"[ultra_fast] Step 3 completed in {step3_time:.1f}s ({step3_time/60:.1f} minutes)")

    # Step 4: Save outputs
    print(f"[ultra_fast] Step 4/4: Saving outputs...")
    step4_start = time.time()
    
    random.Random(seed).shuffle(records)
    n_valid = max(1, int(len(records) * valid_ratio))
    valid_recs = records[:n_valid]
    train_recs = records[n_valid:]

    def write_jsonl(path: Path, items: List[Dict]):
        lines = [json.dumps(obj, ensure_ascii=False) for obj in items]
        with open(path, "w", encoding="utf-8") as f:
            f.write('\n'.join(lines) + '\n')

    train_jsonl = out / "train.jsonl"
    valid_jsonl = out / "valid.jsonl"
    write_jsonl(train_jsonl, train_recs)
    write_jsonl(valid_jsonl, valid_recs)

    # Save bm25.json
    bm25_map = {rec["claim"]: [ev["doc_id"] for ev in rec["evidences"]] for rec in records}
    bm25_json = out / "bm25.json"
    with open(bm25_json, "w", encoding="utf-8") as f:
        json.dump(bm25_map, f, ensure_ascii=False)
    
    step4_time = time.time() - step4_start
    total_time = time.time() - start_time
    
    print(f"[ultra_fast] Step 4 completed in {step4_time:.1f}s")
    print(f"[ultra_fast] üéâ ULTRA-FAST COMPLETED! Total: {total_time:.1f}s ({total_time/60:.1f} minutes)")
    print(f"[ultra_fast] Performance: {total_items/(total_time/60):.0f} claims/minute")

    return {
        "corpus_csv": str(corpus_csv),
        "train_jsonl": str(train_jsonl),
        "valid_jsonl": str(valid_jsonl),
        "bm25_json": str(bm25_json),
    }


# GPU-accelerated version if available
def prepare_from_kvjson_gpu(
    in_path: str,
    out_dir: str,
    max_tokens_per_chunk: int = 220,
    topk_bm25: int = 20,
    valid_ratio: float = 0.1,
    seed: int = 42,
) -> Dict[str, str]:
    """
    GPU-accelerated version using CuPy for matrix operations
    """
    if not HAS_CUPY:
        print("‚ö†Ô∏è  GPU not available, falling back to ultra-fast CPU version")
        return prepare_from_kvjson_ultra_fast(
            in_path, out_dir, max_tokens_per_chunk, topk_bm25, valid_ratio, seed, 6, 2000
        )
    
    print("üöÄ Using GPU acceleration!")
    # GPU implementation would go here
    # For now, fallback to ultra-fast version
    return prepare_from_kvjson_ultra_fast(
        in_path, out_dir, max_tokens_per_chunk, topk_bm25, valid_ratio, seed, 6, 2000
    )
